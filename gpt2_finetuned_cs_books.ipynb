{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMYt4NTd7b9JNCE2yVU7cEY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarunsha009/LLM-Learning/blob/main/gpt2_finetuned_cs_books.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vIfeHxehsud"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "IoC4ItP2h9sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        pdf_reader = PdfReader(file)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "book1 = extract_text_from_pdf(\"/content/Building Microservices - Designing Fine-Grained Systems.pdf\")\n",
        "book2 = extract_text_from_pdf(\"/content/Building Microservices.pdf\")\n",
        "book3 = extract_text_from_pdf(\"/content/Clean Architecture A Craftsman Guide to Software Structure and Design.pdf\")\n",
        "book4 = extract_text_from_pdf(\"/content/Clean.Code.A.Handbook.of.Agile.Software.Craftsmanship.pdf\")\n",
        "book5 = extract_text_from_pdf(\"/content/Guru's SDF (1).pdf\")\n",
        "\n",
        "\n",
        "combined_text = book1 + book2 + book3 + book4 + book5\n",
        "\n",
        "with open(\"cs_books.txt\", \"w\") as f:\n",
        "    f.write(combined_text)"
      ],
      "metadata": {
        "id": "oDExUlT3iyPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(combined_text))"
      ],
      "metadata": {
        "id": "k34dk86MjrkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "  text = re.sub(r'\\n', ' ', text)\n",
        "  text = re.sub(r'[^\\w\\s.,;:!?]', '', text)\n",
        "  return text\n",
        "\n",
        "cleaned_text = clean_text(combined_text)\n"
      ],
      "metadata": {
        "id": "IPm7WDQzjvU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
        "\n",
        "chunks = [cleaned_text[i:i+1000] for i in range(0, len(cleaned_text), 1000)]"
      ],
      "metadata": {
        "id": "7_DSQOeWkDkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_dict({\"text\": chunks})\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "    # Set labels = input_ids (shifted for next-token prediction)\n",
        "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].clone()\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "print(tokenized_datasets[0].keys())  # Should show: input_ids, attention_mask, labels"
      ],
      "metadata": {
        "id": "Bzgvj8uEkLc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, TrainingArguments, Trainer\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"gpt2-cs-books\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=5e-5,\n",
        "    fp16=True,\n",
        "    save_steps=10_000,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "-UIcBOBQkXFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt, model, tokenizer, max_length=100):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,       # Increase for more randomness (try 0.7-1.0)\n",
        "        top_k=50,              # Sample from top 50 likely next tokens\n",
        "        top_p=0.95,            # Nucleus sampling: picks from top tokens covering 95% probability\n",
        "        repetition_penalty=1.2,  # Penalize repeated phrases (values >1.0 reduce repetition)\n",
        "        do_sample=True,        # Enable sampling (required for temperature/top_k/top_p)\n",
        "    )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Example prompt about Clean Code\n",
        "prompt = \"The key principles of Clean Code are:\"\n",
        "generated_text = generate_text(prompt, model, tokenizer)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "dryBghydlu9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain the key principles of Clean Code in computer programming:\"\n",
        "generated_text = generate_text(prompt, model, tokenizer)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "nZMs-xDQpYD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"gpt2-finetuned-cs-books\")\n",
        "tokenizer.save_pretrained(\"gpt2-finetuned-cs-books\")"
      ],
      "metadata": {
        "id": "fCZlIW2NqR9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_high_quality_text(prompt, model, tokenizer, max_length=150):\n",
        "    # Encode the input with attention mask\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    ).to(model.device)\n",
        "\n",
        "    # Generate text with better parameters\n",
        "    output = model.generate(\n",
        "        inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,           # Balanced creativity\n",
        "        top_k=40,                  # Focus on top probable tokens\n",
        "        top_p=0.9,                 # Nucleus sampling\n",
        "        repetition_penalty=1.3,     # Reduce repetition\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Usage:\n",
        "prompt = \"Explain the key principles of Clean Code in computer programming:\"\n",
        "print(generate_high_quality_text(prompt, model, tokenizer))"
      ],
      "metadata": {
        "id": "wvv41cBtqgSv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
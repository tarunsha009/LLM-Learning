{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMIUiJOOaBSxl5TXOR7myaa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarunsha009/LLM-Learning/blob/main/learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63QcbrlZGnlX",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade fsspec datasets"
      ],
      "metadata": {
        "id": "5KhSREZzG9Tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "print(raw_datasets)  # See the structure\n",
        "print(\"\\nFirst training example:\", raw_datasets[\"train\"][0])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6_h5t124HCrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFirst training example:\", raw_datasets[\"test\"][0])"
      ],
      "metadata": {
        "id": "y614N7LhHN5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFirst training example:\", raw_datasets[\"validation\"][0])\n"
      ],
      "metadata": {
        "id": "uthMubZNHQZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True, padding=\"max_length\")\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "print(tokenized_datasets)\n"
      ],
      "metadata": {
        "id": "jesaofuEHa7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = raw_datasets['train'][0]\n",
        "print(sample.keys())\n",
        "tokenize_sample = tokenizer(sample[\"sentence1\"], sample[\"sentence2\"], truncation=True, padding=\"max_length\")\n",
        "print(\"Tokenized iutput keys:\", tokenize_sample.keys())\n",
        "print(\"\\nTokenized sentence 1:\", tokenize_sample[\"input_ids\"])\n",
        "print(\"\\nDecoded sentence 1:\", tokenizer.decode(tokenize_sample[\"input_ids\"]))"
      ],
      "metadata": {
        "id": "JNEhVcqGJt0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ],
      "metadata": {
        "id": "q8zHmfH0KaWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "YYdZl-P3Kuyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_model_pari\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01)"
      ],
      "metadata": {
        "id": "qHSYSrniK6AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainee = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "trainee.train()"
      ],
      "metadata": {
        "id": "-CSMS2vzLvNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainee.evaluate()\n",
        "print(results)"
      ],
      "metadata": {
        "id": "ix7h59isTr9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"bert-mrpc-finetuned\")\n",
        "tokenizer.save_pretrained(\"bert-mrpc-finetuned\")"
      ],
      "metadata": {
        "id": "231fcMxTTyK7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}